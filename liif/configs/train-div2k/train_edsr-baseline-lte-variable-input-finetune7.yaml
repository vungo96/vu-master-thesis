train_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/LSDIR/train/HR
      repeat: 1
      #cache: in_memory
      sharded: true
  wrapper:
    name: sr-implicit-downsampled-collate-batch
    args:
      inp_sizes: [48]
      augment: true
      sample_q: 4096
      # limit_scale: 4 # change scale_min
      scale_min: 1
      # scale_max: 12
      plot_scales: true
  collate_batch: true
  batch_size: 32

gradient_accumulation_steps: 1

val_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/div2k/DIV2K_valid_HR
      first_k: 10
      repeat: 160
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled-collate-batch
    args:
      inp_sizes: [48]
      augment: true
      sample_q: 4096
      limit_scale: 1
      # scale_max: 4
  collate_batch: true
  batch_size: 32

data_norm:
  inp: {sub: [0.5], div: [0.5]}
  gt: {sub: [0.5], div: [0.5]}
  # inp_scale_max: 64

model:
  name: lte
  args:
    encoder_spec:
      name: edsr-baseline
      args:
        no_upsampling: true
    imnet_spec:
      name: mlp
      args:
        out_dim: 3
        hidden_list: [256, 256, 256]
    hidden_dim: 256
    # scale_aware_phase: true
    # scale_aware_mlp: true

optimizer:
  name: adam
  args:
    lr: 1.e-4
epoch_max: 200
multi_step_lr:
  milestones: [40, 80, 120, 160]
  gamma: 0.5

pretrained: save_test/_train_edsr-baseline-lte-variable-input-finetune7_sample-4096-scale-1to8-inputs-48-lsdir-cumulative/epoch-200.pth  # change lr

epoch_val: 1
epoch_save: 20